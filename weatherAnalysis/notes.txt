website to use: 

Perfect! Web scraping via `curl` and parsing HTML is exactly what this coursework is designed for. Here's a revised project idea and checklist using direct HTML scraping.

---

### **Revised Project: Daily Weather Scraper & Analytics**

**Objective:** To create an automated system that scrapes weather data from a public weather website, stores it in MySQL, and generates plots.

*   **Data Source:** We'll use **WorldWeatherOnline.com** or **TimeAndDate.com/weather**. These are less restrictive than major sites but still contain the data in HTML.
*   **What to Collect (for a specific city):**
    *   Current Temperature
    *   "Feels Like" Temperature
    *   Weather Condition (e.g., Partly Cloudy)
    *   Humidity
    *   Wind Speed/Direction
    *   Date and Time of collection

---

### **Revised Week-by-Week Checklist (Nov 18th - 28th)**

#### **Week 1: Foundation & Core Script (Nov 18th - 24th)**

**Goal:** Working scraper that extracts data from HTML and inserts into database.

*   **Monday, Nov 18th - Target Identification & Setup**
    *   [ ] Choose target: **https://www.timeanddate.com/weather/malaysia/kuala-lumpur**
    *   [ ] Use `curl` to fetch the page: `curl -s "https://www.timeanddate.com/weather/malaysia/kuala-lumpur" > weather.html`
    *   [ ] **MANUALLY INSPECT** the HTML. Open `weather.html` in a text editor. Search for the temperature, humidity, etc. Note the HTML tags and classes.
    *   [ ] Create GitHub repo, make first commit with `README.md`.

*   **Tuesday, Nov 19th - HTML Parsing with `grep`, `sed`, `awk`**
    *   [ ] Create `weather_scraper.sh`.
    *   [ ] Use tools to extract data. Example for temperature:
        ```bash
        # Get the line containing the temperature, then extract the number
        temp=$(curl -s "URL" | grep -E 'class="h2"' | sed 's/.*>\([-+]*[0-9]*\).*/\1/')
        ```
    *   [ ] Repeat for other fields. This will be the most challenging part - expect to spend time on regex patterns.
    *   [ ] Make script echo all extracted values cleanly.
    *   [ ] **Commit** your parsing script.

*   **Wednesday, Nov 20th - Database Design**
    *   [ ] Design database table. Since we're scraping one page, one table may suffice: `weather_data (id, temp, feels_like, condition, humidity, wind, scraped_at)`
    *   [ ] Create ERD using `draw.io`.
    *   [ ] Write and execute `database_setup.sql`.
    *   [ ] **Commit** ERD and SQL script.

*   **Thursday, Nov 21st - Database Integration**
    *   [ ] Modify `weather_scraper.sh` to take parsed values and construct an `INSERT` SQL command.
    *   [ ] Test manually. Run script and verify data appears in database.
    *   [ ] **Commit** working database integration.

*   **Friday, Nov 22nd - Robust Error Handling**
    *   [ ] Add checks: 
        *   Does `curl` succeed? (Check exit code `$?`)
        *   After parsing, are the variables populated? If `temp` is empty, the website structure might have changed.
    *   [ ] Add informative log messages.
    *   [ ] **Commit** the more robust script.

*   **Weekend, Nov 23rd-24th - Automation**
    *   [ ] Set up **crontab** to run daily: `0 9 * * * /path/to/weather_scraper.sh` (runs at 9 AM daily).
    *   [ ] Let it run at least twice to collect multiple data points.
    *   [ ] **Major Milestone Reached.**

#### **Week 2: Visualization & Report (Nov 25th - 28th)**

**Goal:** Create plotting scripts and start report.

*   **Monday, Nov 25th - Plotting Foundation**
    *   [ ] Create `weather_plotter.sh`.
    *   [ ] Write SQL to export data: `SELECT scraped_at, temp FROM weather_data INTO OUTFILE '/tmp/weather.csv';`
    *   [ ] Create first plot (Temperature over Time) using gnuplot.
    *   [ ] **Commit** plotting script.

*   **Tuesday, Nov 26th - Develop Multiple Plots**
    *   [ ] Create **10 required plots**. Examples:
        1.  Temperature Trend
        2.  Humidity Over Time
        3.  Wind Speed Over Time
        4.  Temperature vs. Humidity (Scatter Plot)
        5.  Daily Average Temperature (if you have enough data)
        6.  Weather Condition Frequency (Bar chart - requires parsing condition text)
        7.  Temperature Difference (Current vs. "Feels Like")
        8.  Hourly Temperature (if you scrape multiple times per day)
        9.  Weekly Min/Max Temperature
        10. Moving Average of Temperature
    *   [ ] **Commit** with new plots.

*   **Wednesday, Nov 27th - Finalize & Test**
    *   [ ] Ensure all 10 plots work and are clearly labeled.
    *   [ ] Take screenshots of all plots.
    *   [ ] Do final commit of all code.
    *   [ ] Create ZIP file with all project files.

*   **Thursday, Nov 28th - Report Draft**
    *   [ ] Start Word document. Write sections:
        *   **Data Collection:** Explain your parsing logic, show example HTML snippets and the commands used to extract data.
        *   **Database:** Explain your table structure, include ERD.
        *   **Git:** Include GitHub URL and commit history screenshot.
    *   [ ] **You're now ahead of schedule!**

---

### **Important Scraping Tips:**

1.  **User Agent:** Some sites block default `curl`. Use:
    ```bash
    curl -s -A "Mozilla/5.0 (compatible; COMP1314-Student-Project)" "URL"
    ```

2.  **Rate Limiting:** Be polite! Don't scrape more than once per hour. Your daily cron is perfect.

3.  **HTML Changes:** Websites change structure. Your error handling from Friday is crucial.

4.  **Alternative Tools:** If `grep/sed/awk` become too complex, consider:
    ```bash
    # Using pup (HTML parser)
    temp=$(curl -s "URL" | pup 'h2 text{}' | head -1)
    
    # Using lynx
    temp=$(lynx -dump "URL" | grep "Temperature")
    ```

